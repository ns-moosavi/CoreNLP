package edu.stanford.nlp.coref.neural;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Properties;
import java.util.Set;

import org.ejml.simple.SimpleMatrix;

import edu.stanford.nlp.coref.CorefProperties;
import edu.stanford.nlp.coref.CorefRules;
import edu.stanford.nlp.coref.data.Dictionaries;
import edu.stanford.nlp.coref.data.Dictionaries.MentionType;
import edu.stanford.nlp.coref.data.Document;
import edu.stanford.nlp.coref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.NeuralUtils;
import edu.stanford.nlp.util.Pair;

/**
 * Extracts string matching, speaker, distance, and document genre features from mentions.
 * @author Kevin Clark
 */
public class CategoricalFeatureExtractor {
<<<<<<< HEAD
	private final Dictionaries dictionaries;
	private final Map<String, Integer> genres;
	private final boolean conll;
	public FeatureFileParser parser;
	public int[] counts;

	public CategoricalFeatureExtractor(Properties props, Dictionaries dictionaries) {
		this.dictionaries = dictionaries;
		conll = CorefProperties.conll(props);
		parser = new FeatureFileParser();
		counts = new int[parser.m_proFeatures.size()];
		
		genres = new HashMap<>();
		boolean english = CorefProperties.getLanguage(props) == Locale.ENGLISH;
		genres.put("bc", 0);
		genres.put("bn", 1);
		genres.put("mz", 2);
		genres.put("nw", 3);
		if (english) {
			genres.put("pt", 4);
		}
		genres.put("tc", english ? 5 : 4);
		genres.put("wb", english ? 6 : 5);
	}

	public SimpleMatrix getPairFeatures(Pair<Integer, Integer> pair, Document document,
			Map<Integer, List<Mention>> mentionsByHeadIndex) {
		Mention m1 = document.predictedMentionsByID.get(pair.first);
		Mention m2 = document.predictedMentionsByID.get(pair.second);
		List<Integer> featureVals = pairwiseFeatures(document, m1, m2, dictionaries, conll);
		SimpleMatrix features = new SimpleMatrix(featureVals.size() + 1, 1);
		for (int i = 0; i < featureVals.size(); i++) {
			features.set(i + 1, featureVals.get(i));
		}
		features = NeuralUtils.concatenate(features,
				encodeDistance(m2.sentNum - m1.sentNum),
				encodeDistance(m2.mentionNum - m1.mentionNum - 1),
				new SimpleMatrix(new double[][] {{
					m1.sentNum == m2.sentNum && m1.endIndex > m2.startIndex ? 1 : 0}}),
					getMentionFeatures(m1, document, mentionsByHeadIndex),
					getMentionFeatures(m2, document, mentionsByHeadIndex),
					featurizeGenre(document));

		// replicating a bug in the python code
		features.set(0, 0, features.get(features.numRows() - 1, 0));
		features = features.extractMatrix(0, features.numRows() - 1, 0, 1);
		return features;
	}

	public static List<Integer> pairwiseFeatures(Document document, Mention m1, Mention m2,
			Dictionaries dictionaries, boolean isConll) {
		String speaker1 = m1.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
		String speaker2 = m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
		List<Integer> features = new ArrayList<>();
		features.add(isConll ? (speaker1.equals(speaker2) ? 1 : 0) : 0);
		features.add(isConll ?
				(CorefRules.antecedentIsMentionSpeaker(document, m2, m1, dictionaries) ? 1 : 0) : 0);
		features.add(isConll ?
				(CorefRules.antecedentIsMentionSpeaker(document, m1, m2, dictionaries) ? 1 : 0) : 0);
		features.add(m1.headsAgree(m2) ? 1 : 0);
		features.add(
				m1.toString().trim().toLowerCase().equals(m2.toString().trim().toLowerCase()) ? 1 : 0);
		features.add(edu.stanford.nlp.coref.statistical.FeatureExtractor.relaxedStringMatch(m1, m2)
				? 1 : 0);
		return features;
	}

//	public static Map<Integer, Map<Integer, List<Integer>>> extendedPairwiseFeatures(Document document, List<Mention> mentionList,
//			Dictionaries dictionaries, boolean isConll) {
//		Map<Integer, Map<Integer, List<Integer>>> allFeatures = new HashMap<Integer, Map<Integer,List<Integer>>>();
//		for (int i = 1; i < mentionList.size(); i++){
//			Mention m1 = mentionList.get(i);
//			List<Mention> allAntecedents = mentionList.subList(0, i);
//			Map<Integer, List<Integer>> anaFeatures = new HashMap<Integer, List<Integer>>();
//			
//			boolean firstRichNominalExactMatcheMet = false;
//			boolean firstRichNominalHeadMatcheMet = false;
//			boolean firstCompatibleHeadMatchMet = false;
//			
//			for (int k = allAntecedents.size()-1; k >= 0; k--){
//				List<Integer> features = new ArrayList<Integer>();
//				Mention m2 = allAntecedents.get(k);
//
//				String speaker1 = m1.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
//				String speaker2 = m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
//				features.add(isConll ? (speaker1.equals(speaker2) ? 1 : 0) : 0);
//				features.add(isConll ?
//						(CorefRules.antecedentIsMentionSpeaker(document, m2, m1, dictionaries) ? 1 : 0) : 0);
//				features.add(isConll ?
//						(CorefRules.antecedentIsMentionSpeaker(document, m1, m2, dictionaries) ? 1 : 0) : 0);
//				features.add(m1.headsAgree(m2) ? 1 : 0);
//				features.add(
//						m1.toString().trim().toLowerCase().equals(m2.toString().trim().toLowerCase()) ? 1 : 0);
//				features.add(edu.stanford.nlp.coref.statistical.FeatureExtractor.relaxedStringMatch(m1, m2)
//						? 1 : 0);
//				boolean nominalRichExactMatch = m1.nominalRichExactMatch(m2);
//				features.add( nominalRichExactMatch ? 1 : 0);
//				features.add(nominalRichExactMatch && !firstRichNominalExactMatcheMet ? 1 : 0);
//				if (nominalRichExactMatch){
//					firstRichNominalExactMatcheMet = true;
//				}			
//				boolean nominalRichHeadMatch = m1.nominalRichHeadMatch(m2); 
//				features.add( nominalRichHeadMatch? 1 : 0);
//				features.add(nominalRichHeadMatch && !firstRichNominalHeadMatcheMet ? 1 : 0);
//				if (nominalRichHeadMatch)
//					firstRichNominalHeadMatcheMet = true;
//				boolean compatibleHeadMatch = m1.compatibleHeadMatch(m2, dictionaries); 
//				features.add(compatibleHeadMatch ? 1 : 0);
//				features.add(compatibleHeadMatch && !firstCompatibleHeadMatchMet ? 1 : 0);
//				if (compatibleHeadMatch)
//					firstCompatibleHeadMatchMet = true;
//				
//				anaFeatures.put(m2.mentionID, features);
//			}
//			allFeatures.put(m1.mentionID, anaFeatures);
//		}
//		return allFeatures;
//	}

	public Map<Integer, Map<Integer, List<Integer>>> extendedPairwiseFeatures(Document document, List<Mention> mentionList,
			Dictionaries dictionaries, boolean isConll) {

		Map<Integer, Map<Integer, List<Integer>>> allFeatures = new HashMap<Integer, Map<Integer,List<Integer>>>();
		for (int i = 1; i < mentionList.size(); i++){
			Mention m1 = mentionList.get(i);
			Set<String> m1AttVals = parser.getMentionFeatures(m1, "ana");

			List<Mention> allAntecedents = mentionList.subList(0, i);
			Map<Integer, List<Integer>> anaFeatures = new HashMap<Integer, List<Integer>>();
			
			boolean[] firstCompatibles = {false, false, false};
			
			for (int k = allAntecedents.size()-1; k >= 0; k--){
				List<Integer> features = new ArrayList<Integer>();
				Mention m2 = allAntecedents.get(k);
				Set<String> m2AttVals = parser.getMentionFeatures(m2, "ant");
				Set<String> pairAttVals = parser.getPairwiseFeatures(document, m1, m2, dictionaries, firstCompatibles);
				firstCompatibles[0] = firstCompatibles[0]  || pairAttVals.contains("f_comp_obj=True");
				firstCompatibles[1] = firstCompatibles[1] || pairAttVals.contains("f_comp_subj=True");
				firstCompatibles[2] = firstCompatibles[2] || pairAttVals.contains("f_com_preObj=True");
				
				String speaker1 = m1.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
				String speaker2 = m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
				features.add(isConll ? (speaker1.equals(speaker2) ? 1 : 0) : 0);
				features.add(isConll ?
						(CorefRules.antecedentIsMentionSpeaker(document, m2, m1, dictionaries) ? 1 : 0) : 0);
				features.add(isConll ?
						(CorefRules.antecedentIsMentionSpeaker(document, m1, m2, dictionaries) ? 1 : 0) : 0);
				features.add(m1.headsAgree(m2) ? 1 : 0);
				features.add(
						m1.toString().trim().toLowerCase().equals(m2.toString().trim().toLowerCase()) ? 1 : 0);
				features.add(edu.stanford.nlp.coref.statistical.FeatureExtractor.relaxedStringMatch(m1, m2)
						? 1 : 0);
//				
//				if (m1.mentionType == MentionType.NOMINAL){
//					boolean[] nomFeatures = parser.getFeatures(m1AttVals, m2AttVals, pairAttVals, "NOM");
//					for (Boolean b : nomFeatures)
//						features.add(b ? 1 : 0);
//					for (int c = 0; c < parser.m_namFeatures.size()+parser.m_proFeatures.size(); c++)
//						features.add(0);
//				}
//				else if (m1.mentionType == MentionType.PROPER){
//					for (int c = 0; c < parser.m_nomFeatures.size(); c++)
//						features.add(0);
//
//					boolean[] namFeatures = parser.getFeatures(m1AttVals, m2AttVals, pairAttVals, "NAM");
//					for (Boolean b : namFeatures)
//						features.add(b ? 1 : 0);
//					for (int c = 0; c < parser.m_proFeatures.size(); c++)
//						features.add(0);
//				
//				}
				if (m1.mentionType == MentionType.PRONOMINAL){
					for (int c = 0; c < parser.m_nomFeatures.size()+parser.m_namFeatures.size(); c++)
						features.add(0);
					boolean[] proFeatures = parser.getFeatures(m1AttVals, m2AttVals, pairAttVals, "PRO");
					for (int j = 0; j < proFeatures.length; j++){
						features.add(proFeatures[j] ? 1 : 0);
						if (proFeatures[j])
							counts[j]++;
							
					}
				}
				else{
					for (int c = 0; c < parser.m_nomFeatures.size()+parser.m_namFeatures.size()+parser.m_proFeatures.size(); c++)
						features.add(0);
				}

				anaFeatures.put(m2.mentionID, features);
			}
			allFeatures.put(m1.mentionID, anaFeatures);
		}
		return allFeatures;
	}
	
//	private static boolean[] firstRichNominalExactMatch(Mention ana, List<Mention> antecedents){
//		boolean[] ret = new boolean[antecedents.size()];
//		int i = antecedents.size()-1;
//		for (int k = 0; k < antecedents.size(); k++)
//			ret[k] = false;
//
//		boolean found = false;
//		while(i>=0 && !found){
//			if (ana.nominalRichExactMatch(antecedents.get(i))){
//				ret[i] = true;
//				found = true;
//			}
//		}
//		return ret;
//	}
//
//	private static boolean[] firstRichNominalHeadMatch(Mention ana, List<Mention> antecedents){
//		boolean[] ret = new boolean[antecedents.size()];
//		int i = antecedents.size()-1;
//		for (int k = 0; k < antecedents.size(); k++)
//			ret[k] = false;
//
//		boolean found = false;
//		while(i>=0 && !found){
//			if (ana.nominalRichHeadMatch(antecedents.get(i))){
//				ret[i] = true;
//				found = true;
//			}
//		}
//		return ret;
//	}
//
//	private static boolean[] firstCompatibleHeadMatch(Mention ana, List<Mention> antecedents, Dictionaries dict){
//		boolean[] ret = new boolean[antecedents.size()];
//		int i = antecedents.size()-1;
//		for (int k = 0; k < antecedents.size(); k++)
//			ret[k] = false;
//
//		boolean found = false;
//		while(i>=0 && !found){
//			if (ana.compatibleHeadMatch(antecedents.get(i), dict)){
//				ret[i] = true;
//				found = true;
//			}
//		}
//		return ret;
//	}  

	public SimpleMatrix getAnaphoricityFeatures(Mention m, Document document,
			Map<Integer, List<Mention>> mentionsByHeadIndex) {
		return NeuralUtils.concatenate(
				getMentionFeatures(m, document, mentionsByHeadIndex),
				featurizeGenre(document)
				);
	}

	private SimpleMatrix getMentionFeatures(Mention m, Document document,
			Map<Integer, List<Mention>> mentionsByHeadIndex) {
		return NeuralUtils.concatenate(
				NeuralUtils.oneHot(m.mentionType.ordinal(), 4),
				encodeDistance(m.endIndex - m.startIndex - 1),
				new SimpleMatrix(new double[][] {
						{m.mentionNum / (double) document.predictedMentionsByID.size()},
						{mentionsByHeadIndex.get(m.headIndex).stream()
							.anyMatch(m2 -> m != m2 && m.insideIn(m2)) ? 1 : 0}})
				);
	}

	private static SimpleMatrix encodeDistance(int d) {
		SimpleMatrix m = new SimpleMatrix(11, 1);
		if (d < 5) {
			m.set(d, 1);
		} else if (d < 8) {
			m.set(5, 1);
		} else if (d < 16) {
			m.set(6, 1);
		} else if (d < 32) {
			m.set(7, 1);
		} else if (d < 64) {
			m.set(8, 1);
		} else {
			m.set(9, 1);
		}
		m.set(10, Math.min(d, 64) / 64.0);
		return m;
	}

	private SimpleMatrix featurizeGenre(Document document) {
		return NeuralUtils.oneHot(
				conll ? genres.get(document.docInfo.get("DOC_ID").split("/")[0]) : 3, genres.size());
	}
=======
  private final Dictionaries dictionaries;
  private final Map<String, Integer> genres;
  private final boolean conll;

  public CategoricalFeatureExtractor(Properties props, Dictionaries dictionaries) {
    this.dictionaries = dictionaries;
    conll = CorefProperties.conll(props);

    if (conll) {
      genres = new HashMap<>();
      genres.put("bc", 0);
      genres.put("bn", 1);
      genres.put("mz", 2);
      genres.put("nw", 3);
      boolean english = CorefProperties.getLanguage(props) == Locale.ENGLISH;
      if (english) {
        genres.put("pt", 4);
      }
      genres.put("tc", english ? 5 : 4);
      genres.put("wb", english ? 6 : 5);
    } else {
      genres = null;
    }
  }

  public SimpleMatrix getPairFeatures(Pair<Integer, Integer> pair, Document document,
      Map<Integer, List<Mention>> mentionsByHeadIndex) {
    Mention m1 = document.predictedMentionsByID.get(pair.first);
    Mention m2 = document.predictedMentionsByID.get(pair.second);
    List<Integer> featureVals = pairwiseFeatures(document, m1, m2, dictionaries, conll);
    SimpleMatrix features = new SimpleMatrix(featureVals.size(), 1);
    for (int i = 0; i < featureVals.size(); i++) {
      features.set(i, featureVals.get(i));
    }
    features = NeuralUtils.concatenate(features,
        encodeDistance(m2.sentNum - m1.sentNum),
        encodeDistance(m2.mentionNum - m1.mentionNum - 1),
        new SimpleMatrix(new double[][] {{
          m1.sentNum == m2.sentNum && m1.endIndex > m2.startIndex ? 1 : 0}}),
        getMentionFeatures(m1, document, mentionsByHeadIndex),
        getMentionFeatures(m2, document, mentionsByHeadIndex),
        encodeGenre(document));

    return features;
  }

  public static List<Integer> pairwiseFeatures(Document document, Mention m1, Mention m2,
      Dictionaries dictionaries, boolean isConll) {
    String speaker1 = m1.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
    String speaker2 = m2.headWord.get(CoreAnnotations.SpeakerAnnotation.class);
    List<Integer> features = new ArrayList<>();
    features.add(isConll ? (speaker1.equals(speaker2) ? 1 : 0) : 0);
    features.add(isConll ?
        (CorefRules.antecedentIsMentionSpeaker(document, m2, m1, dictionaries) ? 1 : 0) : 0);
    features.add(isConll ?
        (CorefRules.antecedentIsMentionSpeaker(document, m1, m2, dictionaries) ? 1 : 0) : 0);
    features.add(m1.headsAgree(m2) ? 1 : 0);
    features.add(
        m1.toString().trim().toLowerCase().equals(m2.toString().trim().toLowerCase()) ? 1 : 0);
    features.add(edu.stanford.nlp.coref.statistical.FeatureExtractor.relaxedStringMatch(m1, m2)
        ? 1 : 0);
    return features;
  }

  public SimpleMatrix getAnaphoricityFeatures(Mention m, Document document,
      Map<Integer, List<Mention>> mentionsByHeadIndex) {
    return NeuralUtils.concatenate(
        getMentionFeatures(m, document, mentionsByHeadIndex),
        encodeGenre(document)
    );
  }

  private SimpleMatrix getMentionFeatures(Mention m, Document document,
      Map<Integer, List<Mention>> mentionsByHeadIndex) {
    return NeuralUtils.concatenate(
        NeuralUtils.oneHot(m.mentionType.ordinal(), 4),
        encodeDistance(m.endIndex - m.startIndex - 1),
        new SimpleMatrix(new double[][] {
          {m.mentionNum / (double) document.predictedMentionsByID.size()},
          {mentionsByHeadIndex.get(m.headIndex).stream()
            .anyMatch(m2 -> m != m2 && m.insideIn(m2)) ? 1 : 0}})
    );
  }

  private static SimpleMatrix encodeDistance(int d) {
    SimpleMatrix m = new SimpleMatrix(11, 1);
    if (d < 5) {
      m.set(d, 1);
    } else if (d < 8) {
      m.set(5, 1);
    } else if (d < 16) {
      m.set(6, 1);
    } else if (d < 32) {
      m.set(7, 1);
    } else if (d < 64) {
      m.set(8, 1);
    } else {
      m.set(9, 1);
    }
    m.set(10, Math.min(d, 64) / 64.0);
    return m;
  }

  private SimpleMatrix encodeGenre(Document document) {
    return conll ? NeuralUtils.oneHot(
        genres.get(document.docInfo.get("DOC_ID").split("/")[0]), genres.size()) :
          new SimpleMatrix(1, 1);
  }
>>>>>>> 82e432cdbb859a15bbc0863e099ba09af7023109
}
